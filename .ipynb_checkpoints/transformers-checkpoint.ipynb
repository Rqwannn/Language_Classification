{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d00040-e638-455b-84ad-0f06aec4d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ProsusAI/finbert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b09eb-ac4e-4216-814a-1ec152fe4e4d",
   "metadata": {},
   "source": [
    "from_pretrained() adalah sebuah method pada library PyTorch yang digunakan untuk menginisialisasi model dengan menggunakan parameter yang telah dilatih sebelumnya (pretrained weights) pada dataset yang besar. Fungsi ini berguna untuk menghemat waktu dan sumber daya dalam proses pelatihan (training) model pada dataset yang sama atau mirip dengan dataset yang digunakan untuk melatih model sebelumnya. from_pretrained() umumnya digunakan dalam transfer learning atau fine-tuning model untuk tugas tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199a16b2-e037-4ec3-a6b2-13d2c1a100fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# initialize the tokenizer for BERT models\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# initialize the model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc19b1e-9b81-4afd-a59b-9395bf0f6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a975f80-7666-42f9-b9a5-fbc003d8afea",
   "metadata": {},
   "source": [
    "1. Tokenize\n",
    "\n",
    "2. Token IDs -> model\n",
    "\n",
    "3. Model activations -> probabilities ( using Softmax )\n",
    "\n",
    "4. Argmax of those probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7fcbb88-1eb6-4ef0-9750-f0a4311745cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our example text\n",
    "\n",
    "txt = (\"Given the recent downturn in stocks especially in tech which is likely to persist as yields keep going up, \"\n",
    "       \"I thought it would be prudent to share the risks of investing in ARK ETFs, written up very nicely by \"\n",
    "       \"[The Bear Cave](https://thebearcave.substack.com/p/special-edition-will-ark-invest-blow). The risks comes \"\n",
    "       \"primarily from ARK's illiquid and very large holdings in small cap companies. ARK is forced to sell its \"\n",
    "       \"holdings whenever its liquid ETF gets hit with outflows as is especially the case in market downturns. \"\n",
    "       \"This could force very painful liquidations at unfavorable prices and the ensuing crash goes into a \"\n",
    "       \"positive feedback loop leading into a death spiral enticing even more outflows and predatory shorts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8271858c-dd24-445e-973f-c51aba37266d",
   "metadata": {},
   "source": [
    "tokenizer.encode_plus(txt, max_length=512, truncation=True,padding='max_length', add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "* encode_plus() digunakan untuk mengubah teks input menjadi representasi token.\n",
    "* txt adalah input teks yang ingin di-tokenisasi.\n",
    "* max_length digunakan untuk membatasi panjang maksimum teks input setelah di-tokenisasi.\n",
    "* truncation=True digunakan untuk memotong teks input jika melebihi max_length.\n",
    "* padding='max_length' digunakan untuk menambahkan padding pada teks input yang kurang dari max_length.\n",
    "* add_special_tokens=True digunakan untuk menambahkan token-token khusus seperti [CLS], [SEP], dan [PAD] pada awal, akhir, dan padding dari teks input.\n",
    "* return_tensors='pt' digunakan untuk mengembalikan output dalam bentuk PyTorch tensor.\n",
    "\n",
    "Dalam kode di atas, hasil outputnya akan berupa token yang sudah di-padding dan diubah ke dalam bentuk PyTorch tensor. Hal ini berguna untuk digunakan pada model NLP yang memerlukan representasi input dalam bentuk tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d464bac-bc8d-4a4d-bb70-eed3a2553a4f",
   "metadata": {},
   "source": [
    "* [PAD]  = 0   = Used to fill empty space when input sequence is shorter than required sequence size for model\n",
    "* [UNK]  = 100 = If a word/character is not found in BERTs vocabulary it will be represented by this unknown token\n",
    "* [CLS]  = 101 = Represents the start of a sequence\n",
    "* [SEP]  = 102 = Seperator token to denote the end of a sequence and as a seperator where there are multiple sequences\n",
    "* [MASK] = 103 = Token used for masking other tokens, used for masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b888c2-ce1d-4540-bdb4-be054a88bf74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2445,  1996,  3522,  2091, 22299,  1999, 15768,  2926,  1999,\n",
       "          6627,  2029,  2003,  3497,  2000, 29486,  2004, 16189,  2562,  2183,\n",
       "          2039,  1010,  1045,  2245,  2009,  2052,  2022, 10975, 12672,  3372,\n",
       "          2000,  3745,  1996, 10831,  1997, 19920,  1999, 15745,  3802, 10343,\n",
       "          1010,  2517,  2039,  2200, 19957,  2011,  1031,  1996,  4562,  5430,\n",
       "          1033,  1006, 16770,  1024,  1013,  1013,  1996,  4783,  2906, 27454,\n",
       "          1012,  4942,  9153,  3600,  1012,  4012,  1013,  1052,  1013,  2569,\n",
       "          1011,  3179,  1011,  2097,  1011, 15745,  1011, 15697,  1011,  6271,\n",
       "          1007,  1012,  1996, 10831,  3310,  3952,  2013, 15745,  1005,  1055,\n",
       "          5665, 18515, 21272,  1998,  2200,  2312,  9583,  1999,  2235,  6178,\n",
       "          3316,  1012, 15745,  2003,  3140,  2000,  5271,  2049,  9583,  7188,\n",
       "          2049,  6381,  3802,  2546,  4152,  2718,  2007,  2041, 12314,  2015,\n",
       "          2004,  2003,  2926,  1996,  2553,  1999,  3006,  2091, 22299,  2015,\n",
       "          1012,  2023,  2071,  2486,  2200,  9145, 28763,  2015,  2012,  4895,\n",
       "          7011, 14550,  3085,  7597,  1998,  1996, 13831,  5823,  3632,  2046,\n",
       "          1037,  3893, 12247,  7077,  2877,  2046,  1037,  2331, 12313,  4372,\n",
       "          4588,  2075,  2130,  2062,  2041, 12314,  2015,  1998, 21659,  9132,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer.encode_plus(txt, max_length=512, truncation=True,padding='max_length',\n",
    "                               add_special_tokens=True, return_tensors='pt') # tf (tensorflow) replace pt (pytorch)\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf2c88-237c-497c-8eb8-84586bb12f81",
   "metadata": {},
   "source": [
    "without **kwargs\n",
    "\n",
    "random_func(var1='hello', var2='world')\n",
    "\n",
    "with **kwargs\n",
    "\n",
    "input_dict = {'var1': 'hello', 'var2': 'world'}\n",
    "random_func(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdd8f058-34bf-4dad-9d6c-612b42ffc951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.8200,  2.4484,  0.0216]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**token)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f8011e-b397-4390-bd78-fa153716739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8200,  2.4484,  0.0216]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Tensor\n",
    "\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af664509-9bb6-45a9-b8e5-998922d16423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0127, 0.9072, 0.0801]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax to the logits output tensor of our model (in index 0) across dimension -1\n",
    "\n",
    "probs = F.softmax(output[0], dim=-1)\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8516ca0c-bda2-4982-a240-eba2fd3fbfaf",
   "metadata": {},
   "source": [
    "(We use dim=-1 as -1* signifies our tensors final dimension, so if we had a 3D tensor with dims [0, 1, 2] writing dim=-1 is the equivalent to writing dim=2. In this case if we wrote dim=-2 this would be the equivalent to writing dim=1. For a 2D tensor with dims [0, 1], dim=-1 is the equivalent of dim=1.)*\n",
    "\n",
    "Now we have a tensor containing three classes, all with outputs within the probability range of 0-1, these are our probabilities! We can see that class index 1 has the highest probability with a value of 0.9072. We can use PyTorch's argmax function to extract this, we can use argmax after importing torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc232a6-ba83-4b18-b9ad-04e3c4c39f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.argmax(probs)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bc14d-3dfe-49ff-b88b-ac97b35a0f2c",
   "metadata": {},
   "source": [
    "Fungsi dari torch.argmax(probs) adalah untuk mengembalikan indeks dari nilai terbesar di antara tensor probs dalam satu dimensi tertentu. Dalam konteks pemrosesan bahasa alami, probs seringkali berisi probabilitas untuk setiap kelas pada output model, sehingga torch.argmax(probs) akan mengembalikan indeks kelas dengan probabilitas tertinggi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76d646-9cd5-40b3-91ab-006c497e94d2",
   "metadata": {},
   "source": [
    "Argmax outputs our winning class as 1 as expected. To convert this value from a PyTorch tensor to a Python integer we can use the item method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b26a86b9-9a1d-499c-b504-b470d482dc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec83b53-ce2e-4982-9f06-d7c43dd00884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
